{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.smith.langchain.com/tracing/faq/langchain_specific_guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping runs from multi-turn interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using LangChain, you may want to group runs from multi-turn interactions together.\n",
    "\n",
    "With chatbots, copilots, and other common LLM design patterns, users frequently interact with your model over multiple interactions. Each invocation of your model is logged as a separate trace, but you can group these traces together using metadata (see how to add metadata to a run above for more information). Below is a minimal example with LangChain, but the same idea applies when using the LangSmith SDK or API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "That's perfectly fine! Feel free to reach out if you ever have any questions or need assistance in the future. Have a great day!"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈과 클래스를 임포트합니다.\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 대화 프롬프트 템플릿을 생성하고, OpenAI의 GPT-3.5-turbo 모델을 사용하여 대화를 처리하는 체인을 구성합니다.\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful AI.\"),  # 시스템 메시지로 AI의 역할을 정의합니다.\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),  # 대화 기록을 포함하는 자리 표시자입니다.\n",
    "            (\"user\", \"{message}\"),  # 사용자 메시지를 포함합니다. 실제 메시지는 실행 시 결정됩니다.\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\")  # OpenAI의 GPT-3.5-turbo 모델을 사용합니다.\n",
    "    | StrOutputParser()  # 모델의 출력을 문자열로 파싱합니다.\n",
    ")\n",
    "\n",
    "# 대화의 고유 식별자를 설정합니다.\n",
    "conversation_id = \"101e8e66-9c68-4858-a1b4-3b0e3c51a933\"\n",
    "\n",
    "# 대화 기록을 저장할 리스트를 초기화합니다.\n",
    "chat_history = []\n",
    "\n",
    "# 첫 번째 사용자 메시지를 생성합니다.\n",
    "message = HumanMessage(content=\"Hi there\")\n",
    "response = \"\"\n",
    "\n",
    "# 체인을 통해 스트리밍 처리를 수행하고, 결과를 출력합니다.\n",
    "for chunk in chain.stream(\n",
    "    {\n",
    "        \"message\": message,\n",
    "        \"chat_history\": chat_history,\n",
    "    },\n",
    "    config={\"metadata\": {\"conversation_id\": conversation_id}},\n",
    "):\n",
    "    print(chunk, end=\"\")\n",
    "    response += chunk\n",
    "print()\n",
    "\n",
    "# 대화 기록에 사용자 메시지와 AI의 응답을 추가합니다.\n",
    "chat_history.extend(\n",
    "    [\n",
    "        message,\n",
    "        AIMessage(content=response),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 다음 사용자 메시지가 들어올 경우를 대비한 코드입니다.\n",
    "next_message = HumanMessage(content=\"I don't need much assistance, actually.\")\n",
    "for chunk in chain.stream(\n",
    "    {\n",
    "        \"message\": next_message,\n",
    "        \"chat_history\": chat_history,\n",
    "    },\n",
    "    config={\"metadata\": {\"conversation_id\": conversation_id}},\n",
    "):\n",
    "    print(chunk, end=\"\")\n",
    "    response += chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 답은 2개야\n",
      "System: 알겠어요. 그 정보를 기억할게요. 다른 도움이 필요하시면 언제든지 말씀해주세요.\n",
      "Human: 저~기 저~기 산넘고 산넘고 산넘어서 사과나무가 한그루 있다! 거기에 사과가 몇개 열려있게?\n",
      "System: 사과가 몇 개 열려있느냐는 수수께끼인 것 같네요. 정답은 \"2개\"입니다. 혹시 다른 수수께끼나 질문이 있으신가요? 함께 풀어보겠습니다!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈과 클래스를 임포트합니다.\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 대화 프롬프트 템플릿을 생성하고, OpenAI의 GPT-3.5-turbo 모델을 사용하여 대화를 처리하는 체인을 구성합니다.\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful AI.\"),  # 시스템 메시지로 AI의 역할을 정의합니다.\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),  # 대화 기록을 포함하는 자리 표시자입니다.\n",
    "            (\"user\", \"{message}\"),  # 사용자 메시지를 포함합니다. 실제 메시지는 실행 시 결정됩니다.\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\")  # OpenAI의 GPT-3.5-turbo 모델을 사용합니다.\n",
    "    | StrOutputParser()  # 모델의 출력을 문자열로 파싱합니다.\n",
    ")\n",
    "\n",
    "# 대화의 고유 식별자를 설정합니다.\n",
    "conversation_id = \"101e8e66-9c68-4858-a1b4-3b0e3c51a933\"\n",
    "\n",
    "# 대화 기록을 저장할 리스트를 초기화합니다.\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    # 사용자 메시지를 생성합니다.\n",
    "    message = HumanMessage(content=input(\"무엇이든 물어보세요\\n종료를 원하시면 'd'를 입력하세요\"))\n",
    "    if message.content == 'd':\n",
    "        break\n",
    "    print(\"Human:\", message.content)\n",
    "    print(\"System: \", end=\"\")\n",
    "    response = \"\"\n",
    "\n",
    "    # 체인을 통해 스트리밍 처리를 수행하고, 결과를 출력합니다.\n",
    "    for chunk in chain.stream(\n",
    "        {\n",
    "            \"message\": message,\n",
    "            \"chat_history\": chat_history,\n",
    "        },\n",
    "        config={\"metadata\": {\"conversation_id\": conversation_id}},\n",
    "    ):\n",
    "        print(chunk, end=\"\")\n",
    "        response += chunk\n",
    "    print()\n",
    "\n",
    "    # 대화 기록에 사용자 메시지와 AI의 응답을 추가합니다.\n",
    "    chat_history.extend(\n",
    "        [\n",
    "            message,\n",
    "            AIMessage(content=response),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view all the traces from that conversation in LangSmith, you can query the project using a metadata filter:\n",
    "\n",
    "# LangSmith\n",
    "# has(metadata, '{\"conversation_id\":\"101e8e66-9c68-4858-a1b4-3b0e3c51a933\"}')\n",
    "\n",
    "# This will return all traces with the specified conversation ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a run ID from a LangChain call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Typescript, the run ID is returned in the call response under the __run key. In python, we recommend using the run collector callback. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72059deb-dd41-4bbb-99cf-a577a3e19897\n"
     ]
    }
   ],
   "source": [
    "# langchain 라이브러리에서 chat_models, prompts, callbacks 모듈을 임포트합니다.\n",
    "from langchain import chat_models, prompts, callbacks\n",
    "\n",
    "# 대화 체인을 구성합니다.\n",
    "chain = (\n",
    "    prompts.ChatPromptTemplate.from_template(\"Say hi to {name}\")  # 사용자 이름을 포함하는 대화 프롬프트 템플릿을 생성합니다.\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    # | chat_models.ChatAnthropic()  # ChatAnthropic 모델을 사용하여 대화를 처리합니다.\n",
    ")\n",
    "\n",
    "# 대화 실행 결과를 수집하기 위해 콜백을 사용합니다.\n",
    "with callbacks.collect_runs() as cb:\n",
    "    # 체인을 호출하고, 결과를 저장합니다.\n",
    "    result = chain.invoke({\"name\": \"Clara\"})  # 사용자 이름을 Clara로 설정하여 체인을 호출합니다.\n",
    "    run_id = cb.traced_runs[0].id  # 실행된 체인의 ID를 가져옵니다.\n",
    "\n",
    "# 실행된 체인의 ID를 출력합니다.\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi Clara! How are you doing today?' response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None} id='run-3a893d41-447e-4f35-a56f-f11ee1369d4b-0'\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For python LLMs/chat models, the run information is returned automatically when calling the `generate()` method. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('59dff906-2a19-4980-8d46-e5cc62c91173')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    " \n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\") # ChatAnthropic()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a cat\"),\n",
    "        (\"human\", \"Hi\"),\n",
    "    ]\n",
    ")\n",
    "res = chat_model.generate(messages=[prompt.format_messages()])\n",
    "res.run[0].run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='Meow! How can I help you today?', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Meow! How can I help you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-59dff906-2a19-4980-8d46-e5cc62c91173-0'))]] llm_output={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9'} run=[RunInfo(run_id=UUID('59dff906-2a19-4980-8d46-e5cc62c91173'))]\n"
     ]
    }
   ],
   "source": [
    "res\n",
    "\n",
    "'''\n",
    "generations=[[\n",
    "    ChatGeneration(text='Meow! How can I help you today?', \n",
    "                   generation_info={'finish_reason': 'stop', \n",
    "                                    'logprobs': None}, \n",
    "                   message=AIMessage(content='Meow! How can I help you today?', \n",
    "                                     response_metadata={'token_usage': {'completion_tokens': 10, \n",
    "                                                                        'prompt_tokens': 16, \n",
    "                                                                        'total_tokens': 26}, \n",
    "                                                        'model_name': 'gpt-3.5-turbo', \n",
    "                                                        'system_fingerprint': 'fp_d9767fc5b9', \n",
    "                                                        'finish_reason': 'stop', \n",
    "                                                        'logprobs': None}, \n",
    "                                     id='run-59dff906-2a19-4980-8d46-e5cc62c91173-0'))]] \n",
    "llm_output={'token_usage': {'completion_tokens': 10, \n",
    "                            'prompt_tokens': 16, \n",
    "                            'total_tokens': 26}, \n",
    "            'model_name': 'gpt-3.5-turbo', \n",
    "            'system_fingerprint': 'fp_d9767fc5b9'}\n",
    "run=[RunInfo(run_id=UUID('59dff906-2a19-4980-8d46-e5cc62c91173'))]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run-59dff906-2a19-4980-8d46-e5cc62c91173-0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generations[0][0].message.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\r2com\\Desktop\\kpopmap\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7e4c0e15-858e-47b8-9420-d2c67daecd26\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "res = openai.generate([\"You are a good bot\"])\n",
    "print(res.run[0].run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='.\\n\\nThank you! I am programmed to assist and provide helpful responses. Is there anything specific you need assistance with?', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 29, 'prompt_tokens': 5, 'completion_tokens': 24}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('7e4c0e15-858e-47b8-9420-d2c67daecd26'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잘 모르겠음\n",
    "LangSmith 아직 공부 안했고,, 기능도 모르겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing without environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithUserError",
     "evalue": "API key must be provided when using hosted LangSmith API",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLangSmithUserError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from langsmith import Client\u001b[39;00m\n\u001b[0;32m      5\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 6\u001b[0m   \u001b[43mLangChainTracer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYOUR_PROJECT_NAME_HERE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# client=Client(\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#   api_url=\"https://api.smith.langchain.com\",\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#   api_key=\"YOUR_API_KEY_HERE\"\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# )\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI()\n\u001b[0;32m     16\u001b[0m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world!\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks})\n",
      "File \u001b[1;32mc:\\Users\\r2com\\Desktop\\kpopmap\\.venv\\Lib\\site-packages\\langchain_core\\tracers\\langchain.py:91\u001b[0m, in \u001b[0;36mLangChainTracer.__init__\u001b[1;34m(self, example_id, project_name, client, tags, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_id \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     88\u001b[0m     UUID(example_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(example_id, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m example_id\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_name \u001b[38;5;241m=\u001b[39m project_name \u001b[38;5;129;01mor\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mget_tracer_project()\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mget_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags \u001b[38;5;241m=\u001b[39m tags \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_run: Optional[Run] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\r2com\\Desktop\\kpopmap\\.venv\\Lib\\site-packages\\langchain_core\\tracers\\langchain.py:54\u001b[0m, in \u001b[0;36mget_client\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _CLIENT\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _CLIENT \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     _CLIENT \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _CLIENT\n",
      "File \u001b[1;32mc:\\Users\\r2com\\Desktop\\kpopmap\\.venv\\Lib\\site-packages\\langsmith\\client.py:486\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, api_url, api_key, retry_config, timeout_ms, web_url, session, auto_batch_tracing, hide_inputs, hide_outputs, info, api_urls)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m=\u001b[39m _get_api_url(api_url)\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m _get_api_key(api_key)\n\u001b[1;32m--> 486\u001b[0m     \u001b[43m_validate_api_key_if_hosted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_api_urls \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key}\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_config \u001b[38;5;241m=\u001b[39m retry_config \u001b[38;5;129;01mor\u001b[39;00m _default_retry_config()\n",
      "File \u001b[1;32mc:\\Users\\r2com\\Desktop\\kpopmap\\.venv\\Lib\\site-packages\\langsmith\\client.py:275\u001b[0m, in \u001b[0;36m_validate_api_key_if_hosted\u001b[1;34m(api_url, api_key)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_langchain_hosted(api_url):\n\u001b[1;32m--> 275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithUserError(\n\u001b[0;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI key must be provided when using hosted LangSmith API\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m         )\n",
      "\u001b[1;31mLangSmithUserError\u001b[0m: API key must be provided when using hosted LangSmith API"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langsmith import Client\n",
    "\n",
    "callbacks = [\n",
    "  LangChainTracer(\n",
    "    project_name=\"YOUR_PROJECT_NAME_HERE\",\n",
    "    # client=Client(\n",
    "    #   api_url=\"https://api.smith.langchain.com\",\n",
    "    #   api_key=\"YOUR_API_KEY_HERE\"\n",
    "    # )\n",
    "  )\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\", config={\"callbacks\": callbacks})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tactic is also useful for when you have multiple chains running in a shared environment but want to log their run traces to different projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring all traces are submitted before exiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LangChain Python, LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.\n",
    "\n",
    "In LangChain JS, the default is to block for a short period of time for the trace to finish due to the greater popularity of serverless environments. You can make callbacks asynchronous by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"true\".\n",
    "\n",
    "For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "try:\n",
    "    llm.invoke(\"Hello, World!\")\n",
    "finally:\n",
    "    wait_for_all_tracers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
